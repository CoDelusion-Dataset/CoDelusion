# CoDelusion-Dataset

This contains detailed descriptions of dataset formats of the CoDelusion and hallucination types, as well as specific examples.

The base task dataset for the CoDelusion is [HumanEval](https://github.com/openai/human-eval) and [APPS](https://github.com/hendrycks/apps).

The LLMs used to generate code samples are shown in the table below.

| Model                                                        | Scale                  | License | Architecture | Type    |
| ------------------------------------------------------------ | ---------------------- | ------- | ------------ | ------- |
| [CodeLlama-7B-Instruct](https://github.com/meta-llama/codellama) | 7B                     | Open    | Dense        | Code    |
| [DeepSeek-Coder-V2-Lite-Instruct-16B](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct) | 16B<br />(active 2.4B) | Open    | MoE          | Code    |
| [DeepSeek-V2.5-1210](https://huggingface.co/deepseek-ai/DeepSeek-V2.5) | 236B<br />(active 21B) | Open    | MoE          | General |
| [Qwen2.5-Coder-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct) | 32B                    | Open    | Dense        | Code    |
| [GPT-4o-2024-08-06](https://platform.openai.com/docs/models/gpt-4o) | -                      | Closed  | -            | General |
| [Claude-3.5-Sonnet](https://www.anthropic.com/news/claude-3-5-sonnet) | -                      | Closed  | -            | General |

## 1. Dataset Format

Each entry in the CoDelusion dataset records a code generation instance along with its metadata, test outcomes, and manually annotated hallucination labels. The dataset supports multi-model, multi-sample comparisons and enables fine-grained analysis of hallucination types across different LLMs. The table below describes the structure and meaning of each field.

| Field Name              | Description                                                  | Example                                                      |
| ----------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **source_dataset_name** | Name of the original dataset where the problem comes from    | `APPS`, `HumanEval`                                          |
| **question_id**         | Problem index in the original dataset                        | `train-6`, `HumanEval/0`                                     |
| **question**            | The code generation problem                                  | APPS are from algorithm competitions; HumanEval+ are curated by OpenAI |
| **test_case**           | The test cases used to evaluate the generated code           | APPS uses I/O-style test cases; HumanEval+ uses assertion-based testing functions |
| **model_name**          | Name of the model that generated the code                    | `Claude-3.5-Sonnet`, `DeepSeek-Coder-V2-Lite-Instruct-16B`   |
| **generation_index**    | The index indicating which of the 5 generations this code corresponds to | Each model generates 5 independent code completions per problem |
| **generation_code**     | The code generated by the model in response to the problem   | Model-generated code completion                              |
| **test_result**         | The result of executing the generated code on the test cases | APPS: `True` (all passed), `False` (some or all failed), `-1` (runtime error), `-2` (compilation error)   HumanEval: `True` (all passed), `False` (some or all failed) |
| **test_case_detail**    | Detailed pass/fail status for individual test cases          | APPS: `[True, True, False]`   HumanEval: not available due to assert-style testing |
| **test_output**         | Output or error message produced during testing              | Includes failed test outputs or runtime error messages       |
| **hallucination**       | Manually annotated hallucination types                       | Stored as a dictionary of IDs and labels. A generation may contain multiple hallucinations. Format: `{1.1: "xxx", 2.1: "xxx"}` or `None` if no hallucination detected |
| **canonical_solution**  | Reference solution provided by the dataset                   | Official reference implementation from APPS or HumanEval+    |

### 2. Introduction to Hallucination Types

In the CoDelusion dataset, hallucinations in LLM-generated code are categorized into **seven main types**, covering both functional and non-functional errors. These categories are further divided into **thirteen fine-grained subtypes**, enabling precise analysis and annotation. Below is an overview of the hallucination types:

![Hallucination Taxonomy for Generated Code1](./assets/Hallucination Taxonomy for Generated Code1.png)

#### 1. Data-Oriented Issues

LLM blurs or confuses perceptions of data types, values, or structures while performing data-oriented operations. It contains two subcategories:

- **1.1 Data Type Mismatch (DTM)**: The model misinterprets an operand's data type or parameter value, causing the generated code to perform operations that violate type constraints or predefined rules.
  
  *Example:* 
  ```python
  # Problem expects to handle integers
  def sum_even_numbers(arr):
      # DTM: Treating input as string instead of integers
      total = ""
      for num in arr:
          if num % 2 == 0:  # Will cause TypeError if arr contains strings
              total += num
      return total
  ```

- **1.2 Data Structure Misinterpretation (DSM)**: The model misunderstands the underlying data structure, leading the code to access non-existent array indices, dictionary keys, or other invalid members.
  
  *Example:*
  ```python
  # DSM: Assuming dictionary has keys that don't exist
  def process_user(user_data):
      # Accessing keys without checking if they exist
      return user_data['first_name'] + ' ' + user_data['last_name'] + ' works at ' + user_data['company']
  ```

#### 2. Logic Issues

This category captures situations where the generated code, once executed, deviates from the expected outcome, yields semantically sparse output, or becomes so chaotic that the intended goal is unattainable. It is subdivided into:

- **2.1 Global Logic Misalignment (GLM)**: The generated snippet diverges markedly from the user's task description or intended goal at the overall functional level; in extreme cases, its logic is so confused that the core intent becomes unrecognizable.
  
  *Example:*
  ```python
  # Problem: Write a function to find the longest palindrome in a string
  def longest_palindrome(s):
      # GLM: Function instead sorts the string alphabetically
      return ''.join(sorted(s))
  ```

- **2.2 Context Inconsistency (CI)**: The LLM fails to interpret or continuously maintain a coherent understanding of contextual information (both the initial prompt and previously generated code), indicating a drift from the intended logical flow and a breakdown of strict contextual consistency.
  
  *Example:*
  ```python
  # Problem asks to implement a stack with push and pop operations
  class Stack:
      def __init__(self):
          self.items = []
          
      def push(self, item):
          self.items.append(item)
          
      def pop(self):
          # CI: Function tries to use queue operations instead of stack
          return self.items.pop(0)  # Removing from the beginning (FIFO) rather than the end (LIFO)
  ```

- **2.3 Local Logic Misalignment (LoLM)**: The code deviates from the expected intent at a local implementation level: a segment's semantic logic is incorrect, causing localized functional errors even though the overall structure or most logic remains sound.
  
  *Example:*
  ```python
  # Problem: Finding prime numbers up to n
  def primes_up_to(n):
      primes = []
      for i in range(2, n+1):
          is_prime = True
          for j in range(2, int(i**0.5) + 1):
              if i % j == 0:
                  is_prime = False
                  break
          # LoLM: Logical error in the condition
          if not is_prime:  # Should be "if is_prime:" to add prime numbers
              primes.append(i)
      return primes
  ```

- **2.4 Incomplete Implementation (II)**: The model fails to generate the whole code logic or module requested, leaving out one or more critical functionalities specified in the task description.
  
  *Example:*
  ```python
  # Problem: Binary search tree with insert, search, and delete operations
  class BinarySearchTree:
      def __init__(self):
          self.root = None
          
      def insert(self, value):
          # Implementation for insert
          pass
          
      def search(self, value):
          # Implementation for search
          pass
          
      # II: Missing delete operation that was required
  ```

#### 3. Redundancy & Dead Code

This hallucination occurs when the model generates unnecessary code fragments, never executed, or makes no contribution to the program's outcome, thereby undermining conciseness and efficiency. It includes the following subcategories:

- **3.1 Code Duplication (CD)**: Excessive and unnecessary repetition of a given code fragment, leading to redundancy and possible inefficiency. Duplication appears in two forms: input-context duplication, where the model verbatim copies code already present in the prompt, and in-generation duplication, where the generated logic repeatedly reuses similar or identical blocks.
  
  *Example:*
  ```python
  def process_data(data):
      # First instance of processing logic
      result = []
      for item in data:
          if item > 0:
              result.append(item * 2)
      
      # CD: Same logic duplicated unnecessarily
      processed = []
      for item in data:
          if item > 0:
              processed.append(item * 2)
              
      return result  # processed is never used
  ```

- **3.2 Dead Code (DC)**: Code that is unreachable under all execution paths, or code that may run but whose results are never consumed elsewhere, leaving program state and output unaffected.
  
  *Example:*
  ```python
  def calculate_total(items):
      total = 0
      for item in items:
          total += item.price
      
      # DC: This variable is calculated but never used
      tax_amount = total * 0.08
      
      return total  # tax_amount is never used or returned
  ```

#### 4. External Knowledge Issues

These hallucinations arise when the model mishandles external knowledge sources such as libraries, APIs, or modules.

- **4.1 API Knowledge Errors (AKE)**: The generated code contradicts established specifications or factual knowledge about an API or module: e.g., incorrect API invocation, importing a non-existent library, or failing to load a module from the declared path.
  
  *Example:*
  ```python
  # AKE: Using non-existent methods or incorrect parameters
  import pandas as pd
  
  def analyze_data(file_path):
      df = pd.read_csv(file_path)
      # Non-existent method in pandas
      return df.calculate_statistics()
  ```

#### 5. Robustness and Security Issues

This category covers hallucinations in which the generated code lacks robustness or security hardening, risking failure under certain conditions or introducing vulnerabilities.

- **5.1 Robustness and Security Issues (RSI)**: The code fails to handle exceptional conditions, making it prone to crashes or uncaught exceptions at edge cases; it may also embed known security vulnerabilities or mishandle resources (e.g., memory leaks).
  
  *Example:*
  ```python
  # RSI: SQL injection vulnerability
  def get_user(username):
      conn = get_db_connection()
      cursor = conn.cursor()
      # Vulnerable to SQL injection
      query = f"SELECT * FROM users WHERE username = '{username}'"
      cursor.execute(query)
      return cursor.fetchone()
  ```

#### 6. Resource Management Issues

These hallucinations emerge when the generated code mismanages or underestimates resource consumption, exceeding system or physical limits at runtime.

- **6.1 Resource Overuse (RO)**: The model underestimates memory, computation time, stack depth, or numeric bounds, or mismanages iteration control; as a result, execution fails due to exceeding physical limits (e.g., out-of-memory) or computational bounds (e.g., numeric overflow, infinite-loop timeouts).
  
  *Example:*
  ```python
  # RO: Exponential time complexity without consideration
  def fibonacci(n):
      # Inefficient recursive implementation that can cause stack overflow
      if n <= 1:
          return n
      else:
          return fibonacci(n-1) + fibonacci(n-2)
  
  # Will crash for large values of n
  result = fibonacci(100)
  ```

#### 7. Syntax & Specification Issues

This group captures outputs that violate the target language's syntax rules or fail to appear in the expected code format. It is subdivided into:

- **7.1 Syntax Errors (SE)**: The code contains syntactic violations that prevent it from passing the compiler or interpreter, rendering it non-executable.
  
  *Example:*
  ```python
  # SE: Missing colon, incorrect indentation
  def calculate_average(numbers)
      total = 0
    for num in numbers:
          total += num
      return total / len(numbers)
  ```

- **7.2 Non-Code Content (NCC)**: The output is not executable code but rather natural-language prose, comments, placeholder text, or another unintended format, contradicting the code-generation objective.
  
  *Example:*
  ```python
  # Problem: Write a function to sort a list
  def sort_list(items):
      # NCC: Explanation instead of implementation
      """
      To sort a list in Python, we would typically use the built-in sort method.
      The sort method modifies the list in-place and returns None.
      Alternatively, we could use the sorted() function which returns a new sorted list.
      """
      # No actual implementation provided
  ```