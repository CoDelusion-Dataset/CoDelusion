# CodeBERT-based Code Classifier

This project uses Microsoft's CodeBERT pre-trained model for binary classification of code, determining whether code is generated by an LLM (Large Language Model). The classifier makes its determination by analyzing the semantic features of the code and its associated documentation.

## Features

- Uses CodeBERT pre-trained model to extract code semantic features
- Employs the [CLS] token strategy for classification
- Supports balanced dataset processing
- Detailed performance evaluation (accuracy, precision, recall, and F1 score)
- Provides loss curve visualization
- Modular architecture following software engineering best practices

## Requirements

- Python 3.6+
- PyTorch 1.8.0+
- Transformers 4.5.0+
- Other dependencies in requirements.txt

## Quick Start

### Install Dependencies

```bash
pip install -r requirements.txt
```

### Data Preparation

Prepare a CSV format dataset with the following columns:
- `code`: Code text
- `docstring`: Code documentation/description
- `label`: Label (1 for LLM-generated, 0 for human-written)

### Basic Usage

```bash
python main.py --data_path your_data.csv --balance_data --save_model --plot_loss
```

### Advanced Options

Here are some available command-line arguments:

```bash
# Using custom model path and hyperparameters
python main.py --model_path /path/to/codebert-model --epochs 30 --batch_size 32 --learning_rate 2e-5

# Freeze CodeBERT base model for fine-tuning
python main.py --freeze_base_model --data_path your_data.csv

# Custom output directory
python main.py --output_dir ./custom_output --save_model
```

## Parameters

### Data Parameters
- `--data_path`: Path to the data file (default: ./output_utf8.csv)
- `--balance_data`: Whether to balance positive and negative samples
- `--test_size`: Test set proportion (default: 0.2)

### Model Parameters
- `--model_path`: Path to the CodeBERT model (default: ./codebert-base)
- `--freeze_base_model`: Whether to freeze CodeBERT base model parameters

### Training Parameters
- `--max_len`: Maximum sequence length (default: 256)
- `--batch_size`: Batch size (default: 16)
- `--epochs`: Number of training epochs (default: 50)
- `--learning_rate`: Learning rate (default: 3e-4)
- `--warmup_steps`: Number of warmup steps (default: 0)
- `--max_grad_norm`: Gradient clipping threshold (default: 1.0)
- `--num_workers`: Number of data loader workers (default: 4)
- `--seed`: Random seed (default: 42)

### Output Parameters
- `--output_dir`: Output directory (default: ./output)
- `--save_model`: Whether to save the model
- `--plot_loss`: Whether to plot the loss curve

## Project Structure

```
.
├── main.py              # Main entry point
├── config.py            # Configuration parameters
├── data.py              # Data handling functionality
├── model.py             # Model definition
├── utils.py             # Utility functions
├── train.py             # Training functionality
├── __init__.py          # Package initialization
├── requirements.txt     # Dependencies list
├── output/              # Output directory (created automatically)
│   ├── best_model.pth   # Best model weights
│   └── loss_curve.png   # Training loss curve
└── README.md            # Project documentation
```

## Module Descriptions

- **main.py**: Entry point for the application, handles command-line arguments and orchestrates the training pipeline
- **config.py**: Configuration parameters and logging setup
- **data.py**: Dataset classes and data processing functionality
- **model.py**: Model definition and architecture
- **utils.py**: Utility functions including evaluation metrics and result handling
- **train.py**: Training loop and optimization logic

## Model Description

This project uses Microsoft Research's CodeBERT as the base model. CodeBERT is a pre-trained model for programming languages and natural languages that effectively understands code semantics. This project extracts code features using the CodeBERT model and performs classification through a simple fully connected layer.

## License

MIT

## Contact

For any questions, feel free to submit an Issue or PR. 